The following is a digest of the repository "ansible".
This digest is designed to be easily parsed by Large Language Models.

--- SUMMARY ---
Repository: ansible
Files Analyzed: 25
Total Text Size: 24.7 KB
Estimated Tokens (text only): ~7,800

--- DIRECTORY STRUCTURE ---
ansible/
├── group_vars/
│   └── all.yml
├── inventory/
│   └── hosts.ini
├── roles/
│   ├── haproxy/
│   │   └── tasks/
│   │       └── main.yml
│   ├── harbor/
│   │   └── tasks/
│   │       ├── defaults/
│   │       │   └── main.yml
│   │       ├── templates/
│   │       │   └── harbor.yml.j2
│   │       └── main.yml
│   ├── jupyterhub/
│   │   ├── tasks/
│   │   │   └── main.yml
│   │   └── templates/
│   │       ├── nfs-share-pv.yml
│   │       ├── nfs-share-pvc.yml
│   │       └── values.yml
│   ├── k8s_common/
│   │   └── tasks/
│   │       ├── install_crio.yml
│   │       ├── install_kubernetes.yml
│   │       └── main.yml
│   ├── k8s_dashboard/
│   │   └── tasks/
│   │       └── main.yml
│   ├── k8s_master/
│   │   └── tasks/
│   │       └── main.yml
│   ├── k8s_master_join/
│   │   └── tasks/
│   │       └── main.yml
│   ├── k8s_worker/
│   │   └── tasks/
│   │       └── main.yml
│   ├── metrics_server/
│   │   └── tasks/
│   │       └── main.yml
│   ├── monitoring/
│   │   ├── files/
│   │   │   └── values.yaml
│   │   └── tasks/
│   │       └── main.yml
│   └── nfs/
│       └── tasks/
│           └── main.yml
├── ansible.cfg
├── deploy.yml
├── helm_monitoring.yml
└── k8s-setup.yml


--- FILE CONTENTS ---
============================================================
FILE: group_vars/all.yml
============================================================
# dns_servers: "8.8.8.8 1.1.1.1"
control_ip: "192.168.1.173"
pod_cidr: "172.16.1.0/16"
service_cidr: "172.17.1.0/18"
kubernetes_version: "1.30.0-1.1"
kubernetes_version_short: "1.30"
calico_version: "3.26.0"
dashboard_version: "2.7.0"



============================================================
FILE: inventory/hosts.ini
============================================================
[kube_cluster:children]
masters
workers

[nfs]
nfs ansible_host=10.110.188.80 ansible_user=ubuntu ansible_ssh_pass=123 ansible_become_pass=123

[haproxy]
haproxy ansible_host=10.110.188.99 ansible_user=ubuntu ansible_ssh_pass=123 ansible_become_pass=123

[harbor]
harbor ansible_host=10.110.188.77 ansible_user=ubuntu ansible_ssh_pass=123 ansible_become_pass=123

[masters]
master-1 ansible_host=10.110.188.11 ansible_user=ubuntu ansible_ssh_pass=123 ansible_become_pass=123
master-2 ansible_host=10.110.188.12 ansible_user=ubuntu ansible_ssh_pass=123 ansible_become_pass=123
master-3 ansible_host=10.110.188.13 ansible_user=ubuntu ansible_ssh_pass=123 ansible_become_pass=123

[master_init]
master-1 ansible_host=10.110.188.11 ansible_user=ubuntu ansible_ssh_pass=123 ansible_become_pass=123

[master_join]
master-2 ansible_host=10.110.188.12 ansible_user=ubuntu ansible_ssh_pass=123 ansible_become_pass=123
master-3 ansible_host=10.110.188.13 ansible_user=ubuntu ansible_ssh_pass=123 ansible_become_pass=123

[workers]
worker-1 ansible_host=10.110.188.21 ansible_user=ubuntu ansible_ssh_pass=123 ansible_become_pass=123
worker-2 ansible_host=10.110.188.22 ansible_user=ubuntu ansible_ssh_pass=123 ansible_become_pass=123

============================================================
FILE: roles/haproxy/tasks/main.yml
============================================================
- name: Update package cache and fix broken dependencies
  become: yes
  ansible.builtin.shell: |
    apt update -y
    apt --fix-broken install -y
    apt upgrade -y

- name: Install haproxy
  apt:
    name: haproxy
    state: present
    update_cache: true

- name: Configure haproxy for Kubernetes API
  copy:
    dest: /etc/haproxy/haproxy.cfg
    content: |
      global
          daemon
          maxconn 256

      defaults
          mode tcp
          timeout connect 5s
          timeout client  1m
          timeout server  1m

      frontend k8s-api
          bind *:6443
          default_backend k8s-api

      backend k8s-api
      {% for host in groups['masters'] %}
          server master{{ loop.index }} {{ hostvars[host]['ansible_host'] }}:6443 check
      {% endfor %}

- name: Restart haproxy
  service:
    name: haproxy
    state: restarted


============================================================
FILE: roles/harbor/tasks/defaults/main.yml
============================================================
harbor_version: "v2.10.0"
harbor_dir: "/opt/harbor"
harbor_hostname: "{{ inventory_hostname }}"
harbor_host: "{{ ansible_host }}"
harbor_admin_password: "Harbor12345"
harbor_installer_url: "https://github.com/goharbor/harbor/releases/download/{{ harbor_version }}/harbor-online-installer-{{ harbor_version }}.tgz"
harbor_installer_tgz: "{{ harbor_dir }}/harbor-installer.tgz"

============================================================
FILE: roles/harbor/tasks/templates/harbor.yml.j2
============================================================
hostname: {{ harbor_host }}

http:
  port: 80

harbor_admin_password: {{ harbor_admin_password }}

database:
  password: root123

jobservice:
  # Maximum number of job workers in job service
  max_job_workers: 10
  # The jobLoggers backend name, only support "STD_OUTPUT", "FILE" and/or "DB"
  job_loggers:
    - STD_OUTPUT
    - FILE
    # - DB
  # The jobLogger sweeper duration (ignored if `jobLogger` is `stdout`)
  logger_sweeper_duration: 1 #days


notification:
  webhook_job_max_retry: 3
  webhook_job_http_client_timeout: 30

log:
  level: info
  local:
    rotate_count: 50
    rotate_size: 200M
    location: /var/log/harbor


data_volume: /data

enable_chartmuseum: true


============================================================
FILE: roles/harbor/tasks/main.yml
============================================================
- name: Update package cache and fix broken dependencies
  become: yes
  ansible.builtin.shell: |
    apt update -y
    apt --fix-broken install -y
    apt upgrade -y
    
- name: Install Docker and Docker Compose
  apt:
    name: [docker.io, docker-compose]
    state: present
    update_cache: yes

- name: Create Harbor directory
  file:
    path: "{{ harbor_dir }}"
    state: directory

- name: Download Harbor installer
  get_url:
    url: "{{ harbor_installer_url }}"
    dest: "{{ harbor_installer_tgz }}"

- name: Extract Harbor installer
  unarchive:
    src: "{{ harbor_installer_tgz }}"
    dest: "{{ harbor_dir }}"
    remote_src: yes

- name: Template harbor.yml
  template:
    src: harbor.yml.j2
    dest: "{{ harbor_dir }}/harbor/harbor.yml"

- name: Run Harbor install.sh
  ansible.builtin.shell: ./install.sh
  args:
    chdir: "{{ harbor_dir }}/harbor"

- name: Configure Docker to trust insecure Harbor registry
  copy:
    dest: /etc/docker/daemon.json
    content: |
      {
        "insecure-registries": ["{{ harbor_host }}"]
      }
  notify: Restart Docker

# - name: Restart Docker service 
#   shell: systemctl restart docker
#   become: true


============================================================
FILE: roles/jupyterhub/tasks/main.yml
============================================================
- name: Add Helm repo for JupyterHub
  command: helm repo add jupyterhub https://jupyterhub.github.io/helm-chart/

- name: Update Helm repos
  command: helm repo update

- name: Create namespace for JupyterHub
  command: kubectl create namespace jhub --dry-run=client -o yaml | kubectl apply -f -


- name: Apply PersistentVolume for NFS
  copy:
    src: templates/pv-nfs.yaml
    dest: /tmp/pv-nfs.yaml
  register: pv_file

- name: Create PersistentVolume
  command: kubectl apply -f /tmp/pv-nfs.yaml
  when: pv_file is succeeded


- name: Apply PersistentVolumeClaim for NFS
  copy:
    src: templates/pvc-nfs.yaml
    dest: /tmp/pvc-nfs.yaml
  register: pvc_file

- name: Create PersistentVolumeClaim
  command: kubectl apply -f /tmp/pvc-nfs.yaml
  when: pvc_file is succeeded


- name: Install JupyterHub with Helm
  command: >
    helm upgrade --install jhub jupyterhub/jupyterhub
    --namespace jhub
    --version=3.3.7
    -f {{ role_path }}/templates/values.yml

============================================================
FILE: roles/jupyterhub/templates/nfs-share-pv.yml
============================================================
# pv-nfs.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-share-pv
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteMany
  nfs:
    server: {{ nfs_server_ip }}
    path: /export/jupyterhub
  storageClassName: nfs



============================================================
FILE: roles/jupyterhub/templates/nfs-share-pvc.yml
============================================================
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs-share-pvc
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 10Gi
  storageClassName: nfs


============================================================
FILE: roles/jupyterhub/templates/values.yml
============================================================
proxy:
  secretToken: "494abb102a8cd51756c64625b74c4e1c0c8f6918e730a1a913dbe5de521c1c94"
  service:
    type: NodePort
    nodePorts:
      http: 32080
      https: 32443

singleuser:
  cpu:
    limit: 1
    guarantee: 0.5
  memory:
    limit: 2G
    guarantee: 1G
  storage:
    type: "static"
    static:
      pvcName: "nfs-share-pvc"
      subPath: 'home/{username}'
  image:
    name: jupyter/minimal-notebook
    tag: latest
  profileList:
    - display_name: "Python Minimal"
      description: "Environnement de base avec Python uniquement."
      default: true
      kubespawner_override:
        image: jupyter/minimal-notebook:latest
        cpu_limit: 1
        cpu_guarantee: 0.5
        mem_limit: 1G
        mem_guarantee: 512Mi

    - display_name: "Data Science"
      description: "Python, R, Julia avec bibliothèques scientifiques (NumPy, Pandas, Scikit-learn)."
      kubespawner_override:
        image: jupyter/datascience-notebook:latest
        cpu_limit: 2
        cpu_guarantee: 1
        mem_limit: 2G
        mem_guarantee: 1G

    - display_name: "TensorFlow"
      description: "Notebook avec TensorFlow pour deep learning."
      kubespawner_override:
        image: tensorflow/tensorflow:latest-jupyter
        cpu_limit: 2
        cpu_guarantee: 1
        mem_limit: 3G
        mem_guarantee: 1.5G

    - display_name: "PyTorch"
      description: "Notebook avec PyTorch et outils de vision/IA."
      kubespawner_override:
        image: pytorch/pytorch:latest
        cpu_limit: 2
        cpu_guarantee: 1
        mem_limit: 3G
        mem_guarantee: 1.5G

    - display_name: "Big Data & Spark"
      description: "Image Spark pour traitement distribué et analyse big data."
      kubespawner_override:
        image: jupyter/all-spark-notebook:latest
        cpu_limit: 3
        cpu_guarantee: 2
        mem_limit: 4G
        mem_guarantee: 2G

hub:
  config:
    JupyterHub:
      authenticator_class: nativeauthenticator.NativeAuthenticator
      admin_users:
        - root
    NativeAuthenticator:
      open_signup: true
      minimum_password_length: 6
      allow_self_approval: true

imagePullSecrets:
  - name: jupyterhub2


============================================================
FILE: roles/k8s_common/tasks/install_crio.yml
============================================================
- name: Install prerequisites
  apt:
    name:
      - software-properties-common
      - curl
      - apt-transport-https
      - ca-certificates
    state: present
    update_cache: yes

- name: Create CRI-O keyring directory
  file:
    path: /etc/apt/keyrings
    state: directory

- name: Add CRI-O apt key
  shell: |
    curl -fsSL https://pkgs.k8s.io/addons:/cri-o:/prerelease:/main/deb/Release.key | \
    gpg --dearmor -o /etc/apt/keyrings/cri-o-apt-keyring.gpg
  args:
    creates: /etc/apt/keyrings/cri-o-apt-keyring.gpg

- name: Add CRI-O repo
  copy:
    dest: /etc/apt/sources.list.d/cri-o.list
    content: |
      deb [signed-by=/etc/apt/keyrings/cri-o-apt-keyring.gpg] https://pkgs.k8s.io/addons:/cri-o:/prerelease:/main/deb/ /

- name: Install CRI-O
  apt:
    name: cri-o
    state: present
    update_cache: yes

- name: Enable and start CRI-O service
  systemd:
    name: crio
    enabled: yes
    state: started


============================================================
FILE: roles/k8s_common/tasks/install_kubernetes.yml
============================================================
- name: Add Kubernetes apt key
  shell: |
    curl -fsSL https://pkgs.k8s.io/core:/stable:/v{{ kubernetes_version_short }}/deb/Release.key | \
    gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
  args:
    creates: /etc/apt/keyrings/kubernetes-apt-keyring.gpg

- name: Add Kubernetes repo
  copy:
    dest: /etc/apt/sources.list.d/kubernetes.list
    content: |
      deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v{{ kubernetes_version_short }}/deb/ /

- name: Install kubelet, kubeadm, kubectl
  apt:
    name:
      - "kubelet={{ kubernetes_version }}"
      - "kubeadm={{ kubernetes_version }}"
      - "kubectl={{ kubernetes_version }}"
    state: present
    update_cache: yes

- name: Install jq (optional but used in scripts)
  apt:
    name: jq
    state: present

- name: Hold kube packages
  command: apt-mark hold {{ item }}
  loop:
    - kubelet
    - kubeadm
    - kubectl
    - cri-o



============================================================
FILE: roles/k8s_common/tasks/main.yml
============================================================
- name: Update package cache and fix broken dependencies
  become: yes
  ansible.builtin.shell: |
    apt update -y
    apt --fix-broken install -y
    apt upgrade -y

- name: Ensure systemd-resolved config directory exists
  file:
    path: /etc/systemd/resolved.conf.d
    state: directory

- name: Disable swap
  shell: |
    swapoff -a
    (crontab -l 2>/dev/null; echo "@reboot /sbin/swapoff -a") | crontab -
  args:
    executable: /bin/bash

- name: Load required kernel modules
  copy:
    dest: /etc/modules-load.d/k8s.conf
    content: |
      overlay
      br_netfilter

- name: Apply kernel modules
  shell: |
    modprobe overlay
    modprobe br_netfilter

- name: Set sysctl params
  copy:
    dest: /etc/sysctl.d/k8s.conf
    content: |
      net.bridge.bridge-nf-call-iptables = 1
      net.bridge.bridge-nf-call-ip6tables = 1
      net.ipv4.ip_forward = 1

- name: Apply sysctl settings
  command: sysctl --system

- name: Add Kubernetes GPG key
  shell: |
    mkdir -p /etc/apt/keyrings
    curl -fsSL https://pkgs.k8s.io/core:/stable:/v{{ kubernetes_version_short }}/deb/Release.key | \
      gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
  args:
    creates: /etc/apt/keyrings/kubernetes-apt-keyring.gpg

- name: Add Kubernetes apt repository
  copy:
    dest: /etc/apt/sources.list.d/kubernetes.list
    content: |
      deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v{{ kubernetes_version_short }}/deb/ /


- import_tasks: install_crio.yml
- import_tasks: install_kubernetes.yml


============================================================
FILE: roles/k8s_dashboard/tasks/main.yml
============================================================

- name: Create namespace for Kubernetes dashboard
  become_user: "{{ ansible_user }}"
  command: kubectl create namespace kubernetes-dashboard
  ignore_errors: yes

- name: Ensure python3-pip is installed on master
  apt:
    name: python3-pip
    state: present
    update_cache: yes
  become: yes

- name: Install kubernetes python package on master
  pip:
    name: kubernetes
    executable: pip3
  become: yes

- name: Apply dashboard service account
  become_user: "{{ ansible_user }}"
  kubernetes.core.k8s:
    state: present
    definition:
      apiVersion: v1
      kind: ServiceAccount
      metadata:
        name: admin-user
        namespace: kubernetes-dashboard

- name: Apply dashboard token secret
  become_user: "{{ ansible_user }}"
  kubernetes.core.k8s:
    state: present
    definition:
      apiVersion: v1
      kind: Secret
      type: kubernetes.io/service-account-token
      metadata:
        name: admin-user
        namespace: kubernetes-dashboard
        annotations:
          kubernetes.io/service-account.name: admin-user

- name: Bind admin-user to cluster-admin role
  become_user: "{{ ansible_user }}"
  kubernetes.core.k8s:
    state: present
    definition:
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      metadata:
        name: admin-user
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: ClusterRole
        name: cluster-admin
      subjects:
      - kind: ServiceAccount
        name: admin-user
        namespace: kubernetes-dashboard

- name: Deploy Kubernetes dashboard
  become_user: "{{ ansible_user }}"
  command: kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v{{ dashboard_version }}/aio/deploy/recommended.yaml


============================================================
FILE: roles/k8s_master/tasks/main.yml
============================================================
- name: Stop kubelet service
  systemd:
    name: kubelet
    state: stopped
    enabled: no

- name: Kill kube-apiserver and other Kubernetes control plane processes
  shell: |
    pkill kube-apiserver || true
    pkill kube-controller-manager || true
    pkill kube-scheduler || true
    pkill etcd || true

- name: Stop container runtime (crio)
  systemd:
    name: crio
    state: stopped
    enabled: no

- name: Reset any previous kubeadm state (cleanup)
  shell: kubeadm reset -f
  ignore_errors: yes

- name: Remove kube manifests (if any)
  file:
    path: "{{ item }}"
    state: absent
  loop:
    - /etc/kubernetes/manifests/kube-apiserver.yaml
    - /etc/kubernetes/manifests/kube-controller-manager.yaml
    - /etc/kubernetes/manifests/kube-scheduler.yaml
    - /etc/kubernetes/manifests/etcd.yaml

- name: Remove etcd data directory
  file:
    path: /var/lib/etcd
    state: absent

- name: Kill process using port 10257
  shell: |
    fuser -k 10257/tcp || true

- name: Enable and start crio
  systemd:
    name: crio
    state: started
    enabled: yes

- name: Enable and start kubelet
  systemd:
    name: kubelet
    state: started
    enabled: yes

- name: Initialize Kubernetes master via HAProxy
  shell: |
    kubeadm init \
      --control-plane-endpoint "{{ hostvars[groups['haproxy'][0]]['ansible_host'] }}:6443" \
      --pod-network-cidr=10.244.0.0/16 \
      --cri-socket=unix:///var/run/crio/crio.sock
  register: kubeadm_init
  when: groups['haproxy'] | length > 0

- name: Initialize Kubernetes master (no HAProxy)
  shell: |
    kubeadm init \
      --pod-network-cidr=10.244.0.0/16 \
      --cri-socket=unix:///var/run/crio/crio.sock
  register: kubeadm_init
  when: groups['haproxy'] | length == 0


- name: Create .kube directory
  file:
    path: /home/ubuntu/.kube
    state: directory
    owner: ubuntu
    group: ubuntu
    mode: '0755'

- name: Copy admin.conf
  copy:
    src: /etc/kubernetes/admin.conf
    dest: /home/ubuntu/.kube/config
    remote_src: yes
    owner: ubuntu
    group: ubuntu
    mode: '0644'

- name: Install Flannel CNI
  become: false
  shell: |
    kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
  environment:
    KUBECONFIG: /home/ubuntu/.kube/config

- name: Extract worker join command
  shell: kubeadm token create --print-join-command
  register: join_command

- name: Save worker join command to file
  copy:
    content: "{{ join_command.stdout }} --cri-socket=unix:///var/run/crio/crio.sock"
    dest: /tmp/kubeadm_join_cmd.sh

- name: Generate control-plane join command
  shell: kubeadm token create --print-join-command --certificate-key $(kubeadm init phase upload-certs --upload-certs | tail -1)
  register: cp_join_command

- name: Save control-plane join command
  copy:
    content: "{{ cp_join_command.stdout }} --cri-socket=unix:///var/run/crio/crio.sock"
    dest: /tmp/kubeadm_join_cmd_control_plane.sh



============================================================
FILE: roles/k8s_master_join/tasks/main.yml
============================================================
- name: Reset any previous kubeadm state
  shell: kubeadm reset -f
  ignore_errors: yes

- name: Fetch control-plane join command from master-1
  slurp:
    src: /tmp/kubeadm_join_cmd_control_plane.sh
  delegate_to: "{{ groups['master_init'][0] }}"
  register: join_cmd_encoded
  retries: 5
  delay: 3
  until: join_cmd_encoded is success

- name: Decode and save join command
  copy:
    content: "{{ join_cmd_encoded.content | b64decode }}"
    dest: /tmp/kubeadm_join_cmd.sh
    mode: '0755'

- name: Run join command for control-plane
  shell: bash /tmp/kubeadm_join_cmd.sh
  args:
    executable: /bin/bash
  register: join_result
  retries: 3
  delay: 10
  until: join_result.rc == 0


============================================================
FILE: roles/k8s_worker/tasks/main.yml
============================================================
---
- name: Reset Kubernetes on worker node
  shell: kubeadm reset -f
  ignore_errors: yes

- name: Clean leftover Kubernetes files
  file:
    path: "{{ item }}"
    state: absent
  loop:
    - /etc/kubernetes/pki
    - /etc/kubernetes/kubelet.conf
    - /etc/kubernetes/manifests

- name: Ensure master host variable exists
  assert:
    that:
      - groups['masters'] is defined
      - groups['masters'] | length > 0
    fail_msg: "No master defined in inventory (groups['masters'] is empty)."

- name: Fetch join command from master (slurp)
  slurp:
    src: /tmp/kubeadm_join_cmd.sh
  delegate_to: "{{ groups['masters'][0] }}"
  register: join_cmd_encoded
  retries: 5
  delay: 3
  until: join_cmd_encoded is success

- name: Decode and save join command
  copy:
    content: "{{ join_cmd_encoded.content | b64decode }}"
    dest: /tmp/kubeadm_join_cmd.sh
    mode: '0755'

- name: Run join command
  shell: bash /tmp/kubeadm_join_cmd.sh
  args:
    executable: /bin/bash
  register: join_result
  retries: 3
  delay: 10
  until: join_result.rc == 0

- name: Wait for cluster setup to complete
  pause:
    seconds: 60
    prompt: "Waiting 60 seconds for cluster setup"

============================================================
FILE: roles/metrics_server/tasks/main.yml
============================================================
---
# - name: Wait for all nodes to be Ready
#   command: kubectl get nodes --no-headers
#   register: node_status
#   until: node_status.stdout_lines | select("search", "Ready") | list | length == (groups['masters'] | length + groups['workers'] | length)
#   retries: 20
#   delay: 15
#   tags: precheck

# - name: Wait for all kube-system pods to be Running
#   shell: kubectl get pods -n kube-system --no-headers | awk '{print $3}' | grep -v Running
#   register: kube_system_pods
#   until: kube_system_pods.stdout == ""
#   retries: 20
#   delay: 15
#   tags: precheck

# - name: Wait for etcd to be healthy
#   shell: kubectl exec -n kube-system etcd-{{ inventory_hostname }} -- etcdctl endpoint health
#   register: etcd_health
#   until: '"healthy" in etcd_health.stdout'
#   retries: 10
#   delay: 10
#   ignore_errors: yes
#   tags: precheck

- name: Deploy metrics-server
  become_user: "{{ ansible_user }}"
  command: >
    kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
  environment:
    KUBECONFIG: /home/{{ ansible_user }}/.kube/config

- name: Patch metrics-server deployment to allow insecure TLS
  become_user: "{{ ansible_user }}"
  shell: |
    kubectl patch deployment metrics-server -n kube-system \
      --type='json' \
      -p='[{"op":"add","path":"/spec/template/spec/containers/0/args/-","value":"--kubelet-insecure-tls"}]'
  environment:
    KUBECONFIG: /home/{{ ansible_user }}/.kube/config

- name: Wait for metrics-server to be ready
  become_user: "{{ ansible_user }}"
  shell: kubectl wait --namespace kube-system --for=condition=available deployment/metrics-server --timeout=120s
  environment:
    KUBECONFIG: /home/{{ ansible_user }}/.kube/config


============================================================
FILE: roles/monitoring/files/values.yaml
============================================================
grafana:
  adminPassword: admin
  service:
    type: NodePort
    nodePort: 32001
  ingress:
    enabled: false

prometheus:
  service:
    type: NodePort
    nodePort: 32002
  ingress:
    enabled: false

alertmanager:
  enabled: true
  service:
    type: ClusterIP

defaultRules:
  create: true

kubeStateMetrics:
  enabled: true


============================================================
FILE: roles/monitoring/tasks/main.yml
============================================================
- name: Ensure helm is installed
  command: helm version
  register: helm_installed
  failed_when: helm_installed.rc != 0 and 'version.BuildInfo' not in helm_installed.stdout
  ignore_errors: true

- name: Install helm (if not installed)
  shell: |
    curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash
  when: helm_installed.rc != 0

- name: Ensure pip3 is installed
  apt:
    name: python3-pip
    state: present
    update_cache: yes
  become: true

- name: Install Python Kubernetes client
  pip:
    name: kubernetes
    state: present
  become: true

- name: Create monitoring namespace
  kubernetes.core.k8s:
    api_version: v1
    kind: Namespace
    name: monitoring
    state: present
    kubeconfig: /etc/kubernetes/admin.conf

- name: Add prometheus-community helm repo
  command: helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
  args:
    creates: /root/.cache/helm/repository/prometheus-community-index.yaml

- name: Update helm repo
  command: helm repo update

- name: Copy values.yaml to k8s-master-1
  copy:
    src: "{{ role_path }}/files/values.yaml"
    dest: /tmp/values.yaml

- name: Deploy kube-prometheus-stack via Helm (command workaround)
  command: >
    helm upgrade -i prometheus-stack prometheus-community/kube-prometheus-stack
    --namespace monitoring
    --create-namespace
    --reset-values
    --wait
    --timeout 1200s
    --values /tmp/values.yaml
    --kubeconfig /etc/kubernetes/admin.conf










============================================================
FILE: roles/nfs/tasks/main.yml
============================================================
- name: Update package cache and fix broken dependencies
  become: yes
  ansible.builtin.shell: |
    apt update -y
    apt --fix-broken install -y
    apt upgrade -y
    
- name: Install NFS server
  apt:
    name: nfs-kernel-server
    state: present
    update_cache: true

- name: Create export directory
  file:
    path: /srv/nfs/kubedata
    state: directory
    mode: '0777'
    owner: nobody
    group: nogroup

- name: Configure NFS exports
  copy:
    dest: /etc/exports
    content: |
      /srv/nfs/kubedata *(rw,sync,no_subtree_check,no_root_squash)

- name: Export the shared directories
  command: exportfs -rav

- name: Restart NFS service
  service:
    name: nfs-kernel-server
    state: restarted


============================================================
FILE: ansible.cfg
============================================================
[defaults]
host_key_checking = False


============================================================
FILE: deploy.yml
============================================================
# - name: Sleep before deployment
#   hosts: localhost
#   tasks:
#     - pause:
#         seconds: 60

# - name: Deploy HAProxy
#   hosts: haproxy
#   become: true
#   tasks:
#     - name: Include HAProxy role if group exists
#       include_role:
#         name: haproxy
#       when: groups['haproxy'] | length > 0

# - name: Setup Kubernetes Cluster
#   import_playbook: k8s-setup.yml
#   when: (groups['masters'] | length > 0) and (groups['workers'] | length > 0)

- name: Deploy NFS server
  hosts: nfs
  become: true
  tasks:
    - name: Include NFS role if group exists
      include_role:
        name: nfs
      when: groups['nfs'] | length > 0

- name: Deploy Harbor
  hosts: harbor
  become: true
  tasks:
    - name: Include Harbor role if group exists
      include_role:
        name: harbor
      when: groups['harbor'] | length > 0

- name: Deploy Monitoring (Prometheus & Grafana)
  import_playbook : helm_monitoring.yml
  when: groups['masters'] | length > 0

- name: Deploy JupyterHub
  hosts: masters[0]
  become: true
  tasks:
    - name: Run JupyterHub role on first master only
      include_role:
        name: jupyterhub
      run_once: true
      when: (groups['masters'] | length > 0) and (groups['nfs'] | length > 0)


============================================================
FILE: helm_monitoring.yml
============================================================
- name: Deploy Prometheus & Grafana using Helm
  hosts: masters
  become: true
  roles:
    - monitoring


============================================================
FILE: k8s-setup.yml
============================================================
---
- name: Prepare and install Kubernetes
  hosts: kube_cluster
  become: true
  roles:
    - k8s_common

- name: Initialize first master
  hosts: master_init
  become: true
  roles:
    - k8s_master

- name: Join other masters
  hosts: master_join
  become: true
  roles:
    - k8s_master_join

- name: Join worker nodes
  hosts: workers
  become: true
  roles:
    - k8s_worker

- name: Deploy metrics-server
  hosts: master_init
  become: true
  roles:
    - metrics_server